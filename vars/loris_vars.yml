---
#
# SERVICE-WIDE VARS
# -------------------------------------------------------------
#
# assumes dedicated varnish for ids and hx loris
#####


# assumes inventory_hostname is public ip
# assumes ok to get the first ec2 with tag_service loris if list of 'loris'
hx_varnish_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'hxvarnish' %}\
                {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
hx_varnish_port: '80'

# how to access external ids image server
# if configuring a vagrant cluster, ids is a mock
ids_host: '{{ (ec2_tag_cluster == "vagrant") | ternary("ids.vm", "ids.lib.harvard.edu") }}'
# this is to config varnish, who does not talk ssl
ids_port: '80'

# ids host, as exposed by dedicated varnish
# implies that there is always a dedicated varnish for ids; not able to set the
# same varnish instance for both hx and ids...
ids_varnish_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'idsvarnish' %}\
                {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
ids_varnish_port: '80'


# LORIS VARS
# -------------------------------------------------------------
service_git_repo: 'https://github.com/loris-imageserver/loris.git'
service_git_version: 'development'  # head of development branch

service_name: "loris"
service_user: '{{ service_name }}'
service_group: '{{ service_name }}'


# optional var `loris_resolver`
# you can configure either S3 or FS resolvers; if none defined here,
# the default is FS resolver. If defining both, S3 takes precedence.
#
# `s3bucket_map` maps an url prefix to a s3bucket and optional key prefix
# to build the s3 object id for a requested image. For the example below, the
# request for http://loris.org/my_url_prefix/image.jpg/full/128,/0/default.jpg
# will map to s3 object s3://my-s3-bucketname/loris/images/image.jpg
#
# hxloris s3resolver supports multiple s3buckets
# for details, see https://github.com/nmaekawa/hxloris
#
#
# when using a FSResolver, you can pre-populate the image source dir:
# playbooks can copy source images from a local tar.gz or s3sync from a s3 bucket.
# Below configure one or the other; if both configured, tar.gz wins!
#
# for the example below, when configuring an s3 bucket as image source, the s3
# sync will download from s3://my-s3-bucketname/loris/images/*
#
#loris_resolver:
#  s3:
#    resolver_impl: 'must.be.an.s3resolver.Implementation'
#    s3bucket_map:
#      my_url_prefix:
#        bucket: 'my-s3-bucketname'
#        key_prefix: 'loris/images'
#      other_url_prefix:
#        bucket: 'other-bucketname'
#        key_prefix: 'loris/other-images'
#  fs:
#    download_source:
#      from_tar_gz_path: "/local/path/to/image_sample.tar.gz"
#      from_s3:
#        bucket: 'my-s3-bucketname'
#        key_prefix: 'loris/images'


# check in cloudformation for the device name
#ebs_device: /dev/xvdf
ebs_mountpoint: /opt/hx

rootdir: "{{ ebs_mountpoint }}"
service_rootdir: '{{ rootdir }}/{{ service_name }}'
service_venv_dir: "{{ service_rootdir }}/venv"
# service logs go to syslog
service_log_level: "DEBUG"  # 'DEBUG'|'INFO'|'WARNING'|'ERROR'|'CRITICAL'

service_install_dir: '{{ service_rootdir }}/{{ service_name }}'
service_static_dir: '{{ service_rootdir }}/www'
service_config_dir: '{{ service_rootdir }}/conf'
service_config_path: '{{ service_config_dir }}/{{ service_name }}.conf'
service_bin_dir: '{{ service_rootdir }}/bin'
service_data_dir: '{{ service_rootdir }}/data'
service_tmp_dir: '{{ service_data_dir }}/tmp'
service_cache_dir: '{{ service_data_dir }}/cache'
service_images_dir: '{{ service_data_dir }}/images'


# easier to create dirs with this dict
service_dirs_to_create:
    - '{{ service_rootdir }}'
    - '{{ service_bin_dir }}'
    - '{{ service_static_dir }}'
    - '{{ service_config_dir }}'
    - '{{ service_data_dir }}'
    - '{{ service_tmp_dir }}'
    - '{{ service_cache_dir }}'
    - '{{ service_images_dir }}'


# for nmaekawa.apt
apt_required_packages:
    - 'python3-pip'
      #- 'python3-dev'
    - 'python3-setuptools'
    - 'libjpeg-turbo8-dev'
    - 'libfreetype6-dev'
    - 'liblcms2-dev'
    - 'liblcms2-utils'
    - 'libtiff5-dev'
    - 'libwebp-dev'
    - 'supervisor'
    - 'zlib1g-dev'


# get bucket_map from enivronment when s3resolver
default_s3bucket_map: '{"s3bucket": {"bucket": "my-s3-bucket", "key_prefix": "loris/images"}}'
s3bucket_map: "{{ lookup('env', 'LORIS_S3BUCKET_MAP') | default(default_s3bucket_map, true) }}"
default_download_from_s3: '{"bucket": "my-s3-bucket", "key_prefix": "loris/images", "cache_key_prefix": "loris/cache"}'
#loris_download_from_s3: "{{ lookup('env', 'LORIS_DOWNLOAD_FROM_S3') | default(default_download_from_s3) }}"
loris_download_from_s3: "{{ lookup('env', 'LORIS_DOWNLOAD_FROM_S3') | from_json  }}"
loris_resolver:
#  s3:
#    resolver_impl: 'hxloris.s3resolver.S3Resolver'
#    s3bucket_map: "{{ s3bucket_map }}"
  fs:
    download_source:
#      from_tar_gz_path: '/Volumes/Projects/Dev_Team/images/images_prod_20180824.tar.gz'
      from_s3: "{{ loris_download_from_s3 }}"

# how gunicorn exposes access to loris-hx
# hack for vagrant, didn't figure why 2 ip addr
hx_host: "{% for host in hostvars %}\
            {% if hostvars[host].ec2_tag_service == 'loris' %}\
                    {{ hostvars[host].ec2_private_ip_address }}\
            {% endif %}\
        {% endfor %}"
hx_port: '9090'
service_gunicorn_port: '{{ hx_port }}'
service_gunicorn_host: '{{ hx_host }}'
# workers silent for this many secs are killed and restarted
# must be the same as nginx reverse proxy 'proxy_read_timeout'
gunicorn_timeout_seconds: 300


# for Stouts.users
users_enabled: yes
users_to_install:
    - 'nmaekawa'
    - 'lduarte'
    - '{{ service_user }}'


# VARNISH VARS
# -------------------------------------------------------------

#varnish_enabled_services:
#    - varnish
#    - varnishncsa

varnish_instances:
    hx:
        frontend:  # how clients talk to varnish
            host: '{{ hx_varnish_host }}'
            port: '{{ hx_varnish_port }}'
        backend:
            host: '{{ hx_host }}'
            host_header: '{{ webserver_dns }}'
            port: '{{ hx_port }}'
    ids:
        frontend:
            host: '{{ ids_varnish_host }}'
            port: '{{ ids_varnish_port }}'
        backend:
            host: '{{ ids_host }}'
            host_header: '{{ ids_host }}'
            port: '{{ ids_port }}'


# PROXY VARS
# -------------------------------------------------------------

enable_ssl: '{{ ec2_tag_cluster != "vagrant" }}'

# to be overwritten when aws cluster; used when forward proxying to libraries
#proxy_dns_resolver: '8.8.8.8' # google resolver

# for nginx, talk to image servers via varnish
ids_image_host: '{{ varnish_instances.ids.frontend.host }}'
ids_image_port: '{{ varnish_instances.ids.frontend.port }}'
hx_image_host: '{{ varnish_instances.hx.frontend.host }}'
hx_image_port: '{{ varnish_instances.hx.frontend.port }}'

# att: service_static_dir and the actual static_dir for nginx config might be
# different if proxy is to be in separate instance than the service(loris)
# - you can change this in the playbook, in **vars for include_role**
static_dir: '{{ service_static_dir }}'
nginx_ssl_dest_dir: '/etc/ssl/certs'
cert_dns: 'images'  # this name must match pattern for ssl cert names
# special case for vagrant
vagrant_webserver_dns: 'images.vm'

# for nmaekawa.apt
apt_required_packages_proxy:
    - 'nginx'




